\section{Answering Research Questions}
% Discussion
\subsection{Research Question 1} 
Is it possible to get real-time access to the data we record with the Empatica E4?\\[10pt]
Based upon the findings of the pilot experiment we conducted in section \ref{pilot}, we are able to answer in the affirmative. Using the MATLAB TCP client we built in the scope of this project we were able to derive raw signal data from the Empatica E4, using a wireless Bluetooth connection, at the same rate it was recorded.

\subsection{Research Question 2}  
Is it possible to detect and distinguish different degrees of mental workload, as well as two emotional states, such as pleasant and unpleasant, using common machine learning techniques on three physiological signals that were recorded with the Empatica E4 in an experimental setting?\\[10pt]
%\ref{aadb}
With the methods we described in \ref{datainterpretation} we were able to classify a total of 6 different emotional states, with accuracies as high as 58\% for individual classifiers, and an average of 44 \% for all machine learning algorithms. These values, were derived from individual and collective algorithm performances using the original feature set, with a total of 21 features that were extracted from HRV, GSR and skin temperature.\\
Further, we were able to achieve even higher performances for binary classification tasks, aiming to identify specific emotional states. Considering the average performances of all four algorithms we achieved accuracies of 89\% for stress sessions, 72\% for visual stimulation (i.e. both emotional states), and 91\% for resting intervals using again all 21 features.\\
Addressing the distinction of similar emotional states, we assessed the performances of all four classifiers using data sets, only comprised of session from either visual stimulation or stress sessions. Considering again the full feature set we achieved performances of 77\% for resting and stress conditions, and 45\% for both medium and high stress levels, as well as emotional states.\\
Lastly, we report feature dependent accuracies of 61-69\% for all of the abovementioned classification tasks. These values were computed using the average performances of all four classifiers on the respective feature sets.\\
Considering these results, we conclude that our methods are more sensitive to the presence or absence of a certain state opposed to small or gradient changes. But much like Byrne et al. (1996) we believe this type of "coarse" state assessment not necessarily to be a problem. The application of such a binary assessment of workload or emotion may still provide significant information to adaptive logic (e.g. on-task versus off-task) \cite{Byrne1996}. Therefore, we again answer in the affirmative and believe in the potential of our system for the neuroergonomic assessment in adaptive workplaces. 

\subsection{Research Question 3}  
Which algorithm is best suited for the classification task introduced in RQ2?

We found that overall SVM with 5-fold Crossvalidation performed best when used in combination the reduced time feature set. However, when compared over all feature sets all four algorithms achieve similar results ranging from 65 to 67 \%.

\section{Success and Limitations of Neuroergonomic Assessment}
Although, we achieved accuracies well above chance for the classification of 6 different emotions using our selected features, and reached even higher performances on the binary classification of subsets using a variety of feature combination, we still believe that there is room for improvement. Therefore, in the following section, we will discuss the limitations of the present and the possibilities of future work.

\subsection{Limitations}
\textbf{Signal Processing.}

no geometrical features 
The major advantage of geometric methods lies
in their relative insensitivity to the analytic quality of
the series of NN intervals[20]. The major disadvantage is
the need for a reasonable number of NN intervals to
construct the geometric pattern. In practice, recordings
of at least 20 min (but preferably 24 h) should be used
to ensure the correct performance of the geometric
methods, i.e. the current geometric methods are inappropriate
to assess short-term changes in HRV.
\subsection{Future Work}

\section{credibility}
Considering a constant performance of all classifier on the EvA data set we choose to mistrust accuracy as a measure of quality. Considering the confusion matrix, which revealed faulty decision patterns for most of the algorithms, we think of the learning process as unsuccessful. Regarding the small sample size, as well as the Crossvalidation process as possible reasons.
In particular, k-NN classifier performed exceptionally compared to the others on distinguishing among similar emotional states. We believe this discrepancy arises from the overall better performance of nearest neighbor methods on smaller sample sizes.
\section{Feature Selection}
Create additional features is also possible, see (one of the three studies in related work, i think it was Picard) which achieved great results this way



\section{Effectiveness of signal processing}
we observe some issues in one subject with our pre processing of bvp, due to a extreme reflection peak on every pulse wave causing false peak detection and dismissal of the data. This gives us reason to believe that there is still potential to improve. We can not assess the probability of such a failure because of the limited subject group. Or if in some way it was only circumstantial.

However in the filter function for suitable data segments for feature extraction, based on condition A and B proofed to work well and prevented for faulty data to enter the ML algorithm therefore upholding its integrity.

\section{subject selection and access conditions}
- we had some subjects that came from a scientific background, in particular the SNNU and were due to their work there already exposed to the IAPS. This could have had an influence on the ability to elicit emotion during the respective experimental sequences 
- also some of the subjects stated that they were able to do the experiment, but afterwards stated that they did suffer from bad mood, or a mild headache. Again we can not completely estimate the influence of this on the measurement, or rule out that there is any influence at all

Consequently we need to be more strict with subject admission conditions in the future

\section{Effect of Group size on the success of ML}
some ML algorithms are known to work well on small sample sizes, but there are also others that improve on bigger sample sizes.

Also there is a certain inequality in the distribution of emotional states we elicited during the main experiment e.g. only one true baseline, 2 workload segments and therefore only one for each degree, and only 1 sample for each emotional state. Better representation of each state, will definitively improve the ability to recognize them via ML  

Further Grid search comparison between 3 fold , 5 fold ,leave one out:
3 u 5 similar with 3 mostly better due to balance related to our small sample size. bigger group size would allow for higher k fold

Leave one out has to be ruled out due to extremely long training time on the more complex algorithms but seems to be suitable for more simpler ones.

\section{Improvement of emotional segments}
Maybe we could achieve better results on emotional state classification, if we only consider the presentation time of the measurement.

Also our selection of stimuli did not resonate well with some of the subjects, meaning that their valence arousal rating  varied vastly from our expectations.

Therefore  we are uncertain if the right emotion was elicited during the measurement. This would mean that the low success rate in classifying the two emotional states could have its roots in the missing difference. This would also explain why the overall accuracy on detecting emotional states opposed to the rest of the segments was constantly high in all ML algorithms but the accuracy in inter-emotion classification was generally low.

This problem could also be caused by the small sample size, meaning that measuring a far greater subject group may actually represent the intended two emotional states and therefore cause better results with our ML algorithms

Also if data overlaps to much for different workload degrees as well as similar emotional states methods for unsupervised learning (principal component analysis) or linear discriminant analysis may be successful.

Emotion elicitation paradigm: maybe better next time to provide 3 different sets, neutral, happy, unhappy...we used weak happy and unhappy which were rated as neutral.. therefore the 2 emotional states may overlap too much
\section{Recording time}
task force suggest 5 minute recordings, but necessary are only 1 min for HF and 2 min for LF, as the recording interval should be at least 10 times the length of the wavelength of the lowest frequency component.
Therefore, we may be able to achieve more precise classification results by reducing the recording interval. Pro, this would make a system feel more adaptive and reduce processing time for each decision. The effects on the evaluation of frequency features should be tested, as task force suggests to prefer frequency domain methods for short time analysis.
\section{Suitability of ML algorithms}
We touched upon the influence of sample size on the accuracy of ML algorithms but we also have to consider training times, which show substantial differences among some of the algorithms.

Feature selection has noticeable influence on accuracy. Except for variations in bvp features..the accuracy is mostly consistent when they are used which leads to the assumption that they are dominant.

But we were able to achieve good result with a reduced set of features and for some of the binary classifications even with only gsr and temp features.

We propose to try the method of combining a lot of simple binary classifiers and fuse them together in a linear fashion (like the Chinese fellows did) quote paper and reference their results

\section{suitability of E4 - Motion Artifacts}
overall a good device but the reliance on optical sensors still causes a lot of motion artifacts. expecially the large disturbances caused even by miniscule muscle tremors of fingers could lead to long intervals of measurement that cant be used for classification. This requires an elaborate data collection system and extremely strict pre-processing, as well as signal admission.

We propose a more adaptive algorithm that recognizes motion artifacts and instead of filtering them shifts the weight of the signals in the classification algorithm in a way that if a signal suffers from heavy artifacts in a certain time interval its influence is reduced by adjusting the weight and therefor other signal tribute more to the classification instead..until no artifact is recognized for a certain period of time. If we consider the model of fusion from the chinese guys that is

Otherwise it would be wise to explore contact free forms of emotion recognition, although their implementation may present some obstacles too.

\section{Environmental Effects}
To minimize environmental effects we conducted the experiment in laboratories with controlled climate. Although this is general procedure for GSR and skin temperature this approach is generally far from real life application, this could therefore provide a false representation and render these measures useless in a real setting.

Further we darkened the room which leads to similar arguments regarding PPG sensors.

Although we tried to create similar conditions in both labs there still may be some differences remaining, resulting from changes in lighting or AC ( which we could not control in the green lab). This could mean that our data base is split and therefore even smaller. Rendering it obsolete for ML. We advise future studies to account for that.

