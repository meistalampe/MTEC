
% Zusammenfassung

In den letzten Jahren hat eine wachsende Zahl von Forschungen zur Mensch Roboter Interaktion von den Vorteilen der signalbasierten Emotionserkennung sowohl für die Systemleistung als auch für die Gesundheit des Benutzers berichtet. Die überwiegende Mehrheit dieser Studien beschränkte sich jedoch auf künstliche Umgebungen, in denen physiologische Maßnahmen angewandt wurden, die aufgrund ihrer einschränkenden Natur nicht für eine realistische Anwendung geeignet sind.
Die vorliegende Arbeit stellt einen neuartigen Ansatz zur unauffälligen Emotionserkennung in der Mensch-Roboter-Interaktion vor. Auf der Grundlage der  Empatica E4, einem Wearable für medizinische Zwecke, haben wir ein drahtloses Überwachungssystem zur merkmalsbasierten Emotionsklassifizierungen entwickelt.
Um das System aufzubauen, haben wir zunächst ein Datenerfassungsexperiment durchgeführt, bei dem drei physiologische Signale (BVP, GSR und Hauttemperatur) von 14 Personen aufgezeichnet wurden. Während des Experiments mussten die Teilnehmer zunächst zwei kognitive Aufgaben ausführen (Mental Arithmetic Stress Test, STROOP Test) und anschließend zwei Sitzungen mit visueller Stimulation absolvieren. Jede Sitzung verwendete dabei eine Reihe von Bildern, die speziell ausgewählt wurden (aus dem International Affective Picture System), um einen bestimmten emotionalen Zustand hervorzurufen.
Anschließend implementierten wir Feature-Extraktionsalgorithmen für alle Signale und erstellten eine Vielzahl von Datensätze und Feature-Sets für den Lernprozess.
Zuletzt bewerteten wir die Leistung von vier Algorithmen für maschinelles Lernen (k-Nearest Neighbor, Support Vector Machines, Random Forest Classifier und Neural Networks) auf der Grundlage ihrer Klassifizierungsgenauigkeit.
Mit diesem Ansatz waren wir in der Lage 6 verschiedene emotionale Zustände mit Genauigkeiten von bis zu 58\% für einzelne Klassifikatoren und durchschnittlich 44\% für alle Algorithmen des maschinellen Lernens zu klassfizieren.
Darüber hinaus erzielten wir bei binären Klassifizierungsaufgaben, zur Identifikation einzelner emotionaler Zustände, Genauigkeiten von 89\% für Stress Tests, 72\% für visuelle Stimulationstests und 91\% für Ruheintervalle.
Angesichts dieser Ergebnisse haben wir großes Vertrauen in die Anwendbarkeit unseres Systems und in seine Fähigkeit, die Forschung zur signalbasierten Emotionserkennung in der Mensch-Roboter-Interaktion voranzutreiben.